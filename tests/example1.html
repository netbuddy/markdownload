<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">
<head>
    <title>The life of an Ollama prompt - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="https://eli.thegreenplace.net/favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="https://eli.thegreenplace.net/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="https://eli.thegreenplace.net/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="https://eli.thegreenplace.net/theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="https://eli.thegreenplace.net/theme/css/style.css" type="text/css"/>

        <link href="https://eli.thegreenplace.net/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://eli.thegreenplace.net/" class="navbar-brand">
                <img src="https://eli.thegreenplace.net/images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="https://eli.thegreenplace.net/pages/about">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="https://eli.thegreenplace.net/pages/projects">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="https://eli.thegreenplace.net/archives/all">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://eli.thegreenplace.net/2024/the-life-of-an-ollama-prompt/"
                       rel="bookmark"
                       title="Permalink to The life of an Ollama prompt">
                        The life of an Ollama prompt
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> March 06, 2024 at 05:28</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="https://eli.thegreenplace.net/tag/go">Go</a>
        ,
    <a href="https://eli.thegreenplace.net/tag/machine-learning">Machine Learning</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>In <a class="reference external" href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/">a previous post</a>
I've described how - thanks to standardized tooling - we
could use a locally-running <a class="reference external" href="https://blog.google/technology/developers/gemma-open-models/">Gemma model</a>
from a Go program within hours from its public release.</p>
<p>This post dives into the internals of <a class="reference external" href="https://ollama.com/">Ollama</a> -
a popular and extremely convenient open-source Go project that makes such
workflows possible.</p>
<img alt="Mechanical llama being taken apart" class="align-center" src="https://eli.thegreenplace.net/images/2024/mllama.png" />
<div class="section" id="http-request-to-ollama">
<h2>HTTP request to Ollama</h2>
<p>Having <a class="reference external" href="https://ollama.com/download">installed Ollama</a> and
run <tt class="docutils literal">ollama run gemma</tt>, we're ready to send HTTP requests to it. There are
several ways to do so:</p>
<ol class="arabic simple">
<li>Sending a raw HTTP request with a tool like <tt class="docutils literal">curl</tt></li>
<li>Using Ollama's own client libraries (currently available in Go, Python and
JS)</li>
<li>Using a provider-agnostic client like LangChainGo</li>
</ol>
<p>For options (2) and (3) see the Appendix; here we'll focus on (1) for
simplicity and to remove layers from the explanation.</p>
<p>Let's send an HTTP request to the <tt class="docutils literal">api/generate</tt> endpoint of Ollama with
<tt class="docutils literal">curl</tt>:</p>
<div class="highlight"><pre><span></span>$ curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;gemma&quot;,
  &quot;prompt&quot;: &quot;very briefly, tell me the difference between a comet and a meteor&quot;,
  &quot;stream&quot;: false
}&#39; | jq .

[...]

{
  &quot;model&quot;: &quot;gemma&quot;,
  &quot;created_at&quot;: &quot;2024-03-04T14:43:51.665311735Z&quot;,
  &quot;response&quot;: &quot;Sure, here is the difference between a comet and a meteor:

  **Comet:**
  - A celestial object that orbits the Sun in a highly elliptical path.
  - Can be seen as a streak of light in the sky, often with a tail.
  - Comets typically have a visible nucleus, meaning a solid core that
    can be seen from Earth.

  **Meteor:**
  - A streak of hot gas or plasma that appears to move rapidly across the sky.
  - Can be caused by small pieces of rock or dust from space that burn up
    in the atmosphere.
  - Meteors do not have a visible nucleus.&quot;,
  &quot;done&quot;: true,
  &quot;context&quot;:
[...]
}
</pre></div>
<p>(The response is JSON and I've reformatted the text for clarity)</p>
<p>Ollama's HTTP API is <a class="reference external" href="https://github.com/ollama/ollama/blob/main/docs/api.md">documented here</a>.
For each endpoint, it lists a description of parameters and the data returned.</p>
</div>
<div class="section" id="ollama-service">
<h2>Ollama service</h2>
<img alt="Internals of ollama, showing service connecting to clients and loading GGUF" class="align-center" src="https://eli.thegreenplace.net/images/2024/ollama-internals.png" />
<p>Ollama itself is a client-server application; when the installation script is
run, it does several things:</p>
<ul class="simple">
<li>Download Ollama binary</li>
<li>Place it in <tt class="docutils literal">$PATH</tt></li>
<li>Run <tt class="docutils literal">ollama serve</tt> as a background service</li>
</ul>
<p>The service checks the value of the <tt class="docutils literal">OLLAMA_HOST</tt> env var to figure out which
host and port to use. The default is port 11434 on <tt class="docutils literal">localhost</tt> (hence you can
see our <tt class="docutils literal">curl</tt> request is made to <tt class="docutils literal">localhost:11434</tt>). It then listens on
the port, presenting the API discussed above.</p>
<p>What's interesting to note is that when we run <tt class="docutils literal">ollama run &lt;model&gt;</tt> from
the command-line, this invokes the Ollama binary in client mode; in this mode,
it sends requests to the service using the same API. For example, here are
two ways to invoke it - interactive:</p>
<div class="highlight"><pre><span></span>$ ollama run gemma
&gt;&gt;&gt; translate naranjo to english
Naranjo translates to Orange in English.

Naranjo is the Spanish word for Orange.

&gt;&gt;&gt; &lt;Ctrl+D&gt;
</pre></div>
<p>And piping to stdin:</p>
<div class="highlight"><pre><span></span>$ echo &quot;translate naranjo to english&quot; | ollama run gemma
Naranjo translates to Orange in English. Orange is the English word equivalent of the word Naranjo.
</pre></div>
<p>In both these cases, the Ollama binary sends an HTTP request to
<a class="reference external" href="http://localhost:11434/api/generate">http://localhost:11434/api/generate</a>, just like the one we've made manually
with <tt class="docutils literal">curl</tt>.</p>
</div>
<div class="section" id="the-generate-api-endpoint">
<h2>The <tt class="docutils literal">generate</tt> API endpoint</h2>
<p>Now that we know where our prompt to Ollama ends up (whether we issue it using
an HTTP request or the Ollama command-line tool), let's see what the
<tt class="docutils literal">generate</tt> API endpoint actually does.</p>
<p>Ollama uses the Gin web framework, and the API route is fairly standard:</p>
<div class="highlight"><pre><span></span><span class="nx">r</span><span class="p">.</span><span class="nx">POST</span><span class="p">(</span><span class="s">&quot;/api/generate&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">GenerateHandler</span><span class="p">)</span><span class="w"></span>
</pre></div>
<p>This routes HTTP POST requests for <tt class="docutils literal">/api/generate</tt> to a handler function called
<tt class="docutils literal">GenerateHandler</tt>, which is defined <a class="reference external" href="https://github.com/ollama/ollama/blob/main/server/routes.go">in the same source file</a>:</p>
<div class="highlight"><pre><span></span><span class="kd">func</span><span class="w"> </span><span class="nx">GenerateHandler</span><span class="p">(</span><span class="nx">c</span><span class="w"> </span><span class="o">*</span><span class="nx">gin</span><span class="p">.</span><span class="nx">Context</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="p">[</span><span class="o">...</span><span class="p">]</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
<p>After parsing and validating the request, <tt class="docutils literal">GenerateHandler</tt> starts by
fetching the model the request asked for with the <tt class="docutils literal">&quot;model&quot;</tt> field. It then
loads the right model and runs it, feeding it with the prompt provided
in the request. The next sections describe these two steps.</p>
</div>
<div class="section" id="fetching-and-loading-the-model">
<h2>Fetching and loading the model</h2>
<p>When Ollama is looking for a model (by name), it first checks if it already
has it downloaded and stored locally. On my Linux machine, Ollama stores its
local cache of models at <tt class="docutils literal"><span class="pre">/usr/share/ollama/.ollama/models/blobs</span></tt>. If the
model is already available locally, there's not much to do for this step.</p>
<p>Otherwise, Ollama looks in its <a class="reference external" href="https://ollama.com/library">online library</a>
of models. Specifically, the service makes a request to
<a class="reference external" href="https://registry.ollama.ai/v2/library/">https://registry.ollama.ai/v2/library/</a> to check if a model exists. At the time
of writing, it's not clear if anyone except the Ollama maintainers can upload
new models to the library - but it seems like they're working on this option.</p>
<p>But where do these models come from? As <a class="reference external" href="https://github.com/ollama/ollama/blob/main/docs/import.md">this doc explains</a>, models are
imported from other sources in formats like GGUF or Safetensors. The topic
of these formats is very interesting, but I won't be covering it in this
post; if you're interested, <a class="reference external" href="https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/">a recent blog post by Vicki Boykis</a>
provides useful historic background.</p>
<p>While models can be imported from a variety of formats, Ollama's library
stores them as GGUF and that's what the service expects to find.</p>
<p>For the purpose of this explanation, it's sufficient to know that GGUF
stores some metadata about the model (e.g. its architecture and parameters,
like numbers of layers in different parts, etc) as well as its actual weights.
The weights can be stored in different formats - some more suitable for
GPUs, some for CPUs. Quantization is common, especially for CPU-oriented models.
The model file is usually a giant multi-GiB binary blob that needs to be
downloaded and cached locally.</p>
</div>
<div class="section" id="running-the-underlying-model-with-a-prompt">
<h2>Running the underlying model with a prompt</h2>
<p>To run the model, Ollama turns to another project - <a class="reference external" href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>. <tt class="docutils literal">llama.cpp</tt> arose as a local
inference engine for the <a class="reference external" href="https://en.wikipedia.org/wiki/LLaMA">Llama model</a>
when it was originally released. Since the model architecture and weights
were published, it became possible to implement inference for the model
without relying on full-blown Python ML frameworks like TensorFlow, PyTorch or
JAX. It uses its author's separate project - <a class="reference external" href="https://github.com/ggerganov/ggml">ggml</a>, for an efficient C++ library of ML
primitives that can run on CPUs and GPUs.</p>
<p>Originally <tt class="docutils literal">llama.cpp</tt> just hard-coded Llama's architecture and loaded the
weights, but in time it grew to incorporate additional open-sourced models
and its implementation became a kind of a <tt class="docutils literal">switch</tt> based on
the model's architecture.</p>
<p>For example, <a class="reference external" href="https://github.com/ggerganov/llama.cpp/commit/580111d42b3b6ad0a390bfb267d6e3077506eb31">this commit</a>
added Gemma support to <tt class="docutils literal">llama.cpp</tt> <a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a>. Once this is in place, all it needs is to
load the weights and some parameterization of the model from its GGUF file and
it's ready to go.</p>
<p><tt class="docutils literal">llama.cpp</tt> is a C++ project that was originally designed as a command-line
utility you can use to load models and chat with them. C++ is not known for
having a pleasant or stable ABI to work with, so many projects wrapped
<tt class="docutils literal">llama.cpp</tt> with a lightweight C ABI in order to create bindings into other
languages.</p>
<p>Ollama, as a Go project, did the same. It went a step further though, and
cleverly leverages <tt class="docutils literal">llama.cpp</tt>'s <a class="reference external" href="https://github.com/ggerganov/llama.cpp/tree/master/examples/server">server sample</a>,
which encapsulates all operations in functions that take JSON inputs and
return JSON outputs. Ollama added some glue in <a class="reference external" href="https://github.com/ollama/ollama/tree/main/llm/ext_server">ext_server</a>,
and wrapped it with <tt class="docutils literal">cgo</tt> to be able to invoke <tt class="docutils literal">llama.cpp</tt> inference
in-process.</p>
<p>The <tt class="docutils literal">generate</tt> endpoint calls <a class="reference external" href="https://pkg.go.dev/github.com/jmorganca/ollama&#64;v0.1.28/llm#LLM.Predict">llm.Predict</a>,
which after some hops ends <tt class="docutils literal">llama.cpp</tt>'s <a class="reference external" href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/server.cpp#L1260">request_completion</a>.</p>
</div>
<div class="section" id="afterword-standard-interfaces">
<h2>Afterword: standard interfaces</h2>
<p>In my <a class="reference external" href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/">previous post</a>,
I've mentioned that the flow works and is easy to set up due to standardized
interfaces that have been implemented in OSS projects.</p>
<p>After reading this post with Ollama internals, I hope it's clear what
standardized interfaces come into play here.</p>
<p>First and foremost is <tt class="docutils literal">llama.cpp</tt> and its associated GGUF format. While the
internals of <tt class="docutils literal">llama.cpp</tt> are somewhat clunky, this project is unapologetically
pragmatic and a true boon for
the ecosystem because of the way it standardizes LLM inference (and embeddings).
Given a model architecture implemented in C++ in the innards of <tt class="docutils literal">llama.cpp</tt>,
variations can be easily explored and run on compatible CPUs and GPUs. Slight
model modifications? Tuning? Trying some new kind of quantizations? Just create
a GGUF file and <tt class="docutils literal">llama.cpp</tt> will run it for you.</p>
<p>The other half of the solution is Ollama, which wraps <tt class="docutils literal">llama.cpp</tt> in a
conveniently packaged tool, API and ecosystem <a class="footnote-reference" href="#footnote-2" id="footnote-reference-2">[2]</a>. As a Go project, it's easily
distributable and makes it trivial to hack on a powerful API server. The REST
API it presents can then be leveraged by any tool capable of issuing HTTP
requests.</p>
</div>
<div class="section" id="appendix-go-client-libraries-for-the-ollama-api">
<h2>Appendix: Go client libraries for the Ollama API</h2>
<p>If you want to use LLMs programmatically from Go through Ollama, the most
convenient options are either using Ollama's own Go client library or through
LangChainGo. Another option - as discussed above - is to send raw HTTP requests.</p>
<p>The <a class="reference external" href="https://pkg.go.dev/github.com/jmorganca/ollama/api">Ollama Go client library</a>
is a great option because it's what the Ollama client itself uses to talk to the
service; it's as battle-tested and functional as you can hope for. On the other
hand, LangChainGo is convenient if you use multiple providers and want code
that's consistent and provider-agnostic.</p>
<p><a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/ollama-go-clients">This sample</a>
lists Go code to ask Ollama a question using (1) the Ollama Go library or (2)
LangChainGo.</p>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[1]</a></td><td>The Gemma announcement points to this official documentation and
implementation - <a class="reference external" href="https://github.com/google-deepmind/gemma">https://github.com/google-deepmind/gemma</a> - it can be
used to re-implement Gemma inference, along with the pre-trained model
weights Google released.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[2]</a></td><td>Ollama has additional capabilities I haven't mentioned here, like <a class="reference external" href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">Modelfiles</a> for creating
and sharing models.</td></tr>
</tbody>
</table>
</div>

            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2024 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="//code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://eli.thegreenplace.net/theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="https://eli.thegreenplace.net/theme/js/count.js"></script>
</body>
</html>